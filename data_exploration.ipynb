{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>265140</td>\n",
       "      <td>382056</td>\n",
       "      <td>120181</td>\n",
       "      <td>What are the benefits of using honey on skin?</td>\n",
       "      <td>Is it true that honey is capable of changing y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>393919</td>\n",
       "      <td>526758</td>\n",
       "      <td>526759</td>\n",
       "      <td>What cars were sold in America with a 2JZ-GTE?</td>\n",
       "      <td>Is it a good choice to join M.tech engineering...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44808</td>\n",
       "      <td>80383</td>\n",
       "      <td>80384</td>\n",
       "      <td>Is there a place between Bangalore and Pune fo...</td>\n",
       "      <td>Is there a place between Bangalore and Pune fo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90064</td>\n",
       "      <td>151197</td>\n",
       "      <td>151198</td>\n",
       "      <td>How can I stop myself from thinking too much a...</td>\n",
       "      <td>How do I stop myself from watching porn and th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6653</td>\n",
       "      <td>13025</td>\n",
       "      <td>13026</td>\n",
       "      <td>How can I stop being boastful?</td>\n",
       "      <td>In you opinion, what is the best Explosions in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    qid1    qid2                                          question1  \\\n",
       "0  265140  382056  120181      What are the benefits of using honey on skin?   \n",
       "1  393919  526758  526759     What cars were sold in America with a 2JZ-GTE?   \n",
       "2   44808   80383   80384  Is there a place between Bangalore and Pune fo...   \n",
       "3   90064  151197  151198  How can I stop myself from thinking too much a...   \n",
       "4    6653   13025   13026                     How can I stop being boastful?   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  Is it true that honey is capable of changing y...             0  \n",
       "1  Is it a good choice to join M.tech engineering...             0  \n",
       "2  Is there a place between Bangalore and Pune fo...             1  \n",
       "3  How do I stop myself from watching porn and th...             1  \n",
       "4  In you opinion, what is the best Explosions in...             0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import sklearn.model_selection\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, Reshape, Merge, BatchNormalization, TimeDistributed, Lambda, LSTM\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "import sklearn.metrics\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "_EPSILON = K.epsilon()\n",
    "\n",
    "df = pd.read_csv(\"../d/train.csv\")\n",
    "df_dev = pd.read_csv(\"../d/dev.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def fit_tokenizer(df, num_words):\n",
    "    questions = np.concatenate([df.question1.values, df.question1.values])\n",
    "    tk = Tokenizer(num_words=num_words, lower=True, split=\" \")\n",
    "    tk.fit_on_texts(questions)\n",
    "    return tk\n",
    "\n",
    "def transform_tokenizer(df, tk):\n",
    "    #maxlen = 256\n",
    "    maxlen = 32\n",
    "    df.question1.fillna(\"\", inplace=True)\n",
    "    df.question2.fillna(\"\", inplace=True)\n",
    "    sq1 = tk.texts_to_sequences(df.question1.values)\n",
    "    sq2 = tk.texts_to_sequences(df.question2.values)\n",
    "    sq1p = pad_sequences(sq1, maxlen=maxlen, dtype='int32', padding='post', truncating='post', value=0.)\n",
    "    sq2p = pad_sequences(sq2, maxlen=maxlen, dtype='int32', padding='post', truncating='post', value=0.)    \n",
    "    return np.array(sq1p), np.array(sq2p)\n",
    "\n",
    "tk = fit_tokenizer(df, 5000)\n",
    "q1_train, q2_train = transform_tokenizer(df, tk)\n",
    "y_train = df.is_duplicate.values\n",
    "q1_dev, q2_dev = transform_tokenizer(df_dev, tk)\n",
    "y_dev = df_dev.is_duplicate.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\taihei\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\__main__.py:20: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "merge_12 (Merge)             (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 716,417.0\n",
      "Trainable params: 716,417\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM=64\n",
    "nb_words = 5000\n",
    "\n",
    "def add_layers(model):\n",
    "\n",
    "    model.add(Embedding(nb_words, EMBEDDING_DIM))\n",
    "    model.add(LSTM(EMBEDDING_DIM, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "def get_model():\n",
    "    EMBEDDING_DIM=512\n",
    "    nb_words = 5000\n",
    "    \n",
    "    model1 = Sequential()\n",
    "    add_layers(model1)\n",
    "    \n",
    "    model2 = Sequential()\n",
    "    add_layers(model2)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Merge([model1, model2], mode='concat'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Dense(200, activation='relu'))\n",
    "    #model.add(BatchNormalization())\n",
    "    \"\"\"\n",
    "    model1 = Sequential()\n",
    "    model1.add(Embedding(nb_words + 1, EMBEDDING_DIM, input_length=256, trainable=False))\n",
    "    model1.add(TimeDistributed(Dense(EMBEDDING_DIM, activation='relu')))\n",
    "    model1.add(Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBEDDING_DIM, )))\n",
    "    \n",
    "    model2 = Sequential()\n",
    "    model2.add(Embedding(nb_words + 1, EMBEDDING_DIM, input_length=256, trainable=False))\n",
    "    model2.add(TimeDistributed(Dense(EMBEDDING_DIM, activation='relu')))\n",
    "    model2.add(Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBEDDING_DIM, )))\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Merge([model1, model2], mode='concat'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Dense(200, activation='relu'))\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Dense(200, activation='relu'))\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Dense(200, activation='relu'))\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Dense(200, activation='relu'))\n",
    "    #model.add(BatchNormalization())\n",
    "    \"\"\"\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def _logloss(y_true, y_pred):\n",
    "    y_pred = K.clip(y_pred, _EPSILON, 1.0-_EPSILON)\n",
    "    out = -(y_true * K.log(y_pred) + (1.0 - y_true) * K.log(1.0 - y_pred))\n",
    "    return K.mean(out, axis=-1)\n",
    "\n",
    "# https://github.com/bradleypallen/keras-quora-question-pairs\n",
    "model = get_model()\n",
    "model.compile(loss=_logloss, optimizer='adam', metrics=['accuracy', _logloss])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 20000 samples\n",
      "Epoch 1/10\n",
      "130s - loss: 0.5877 - acc: 0.6949 - _logloss: 0.5877 - val_loss: 0.5374 - val_acc: 0.7358 - val__logloss: 0.5374\n",
      "Epoch 2/10\n",
      "126s - loss: 0.5263 - acc: 0.7436 - _logloss: 0.5263 - val_loss: 0.5365 - val_acc: 0.7379 - val__logloss: 0.5365\n",
      "Epoch 3/10\n",
      "127s - loss: 0.5046 - acc: 0.7556 - _logloss: 0.5046 - val_loss: 0.5251 - val_acc: 0.7440 - val__logloss: 0.5251\n",
      "Epoch 4/10\n",
      "126s - loss: 0.4891 - acc: 0.7636 - _logloss: 0.4891 - val_loss: 0.5245 - val_acc: 0.7398 - val__logloss: 0.5245\n",
      "Epoch 5/10\n",
      "127s - loss: 0.4768 - acc: 0.7703 - _logloss: 0.4768 - val_loss: 0.5332 - val_acc: 0.7413 - val__logloss: 0.5332\n",
      "Epoch 6/10\n",
      "126s - loss: 0.4641 - acc: 0.7761 - _logloss: 0.4641 - val_loss: 0.5300 - val_acc: 0.7402 - val__logloss: 0.5300\n",
      "Epoch 7/10\n",
      "126s - loss: 0.4513 - acc: 0.7817 - _logloss: 0.4513 - val_loss: 0.5373 - val_acc: 0.7444 - val__logloss: 0.5373\n",
      "Epoch 8/10\n",
      "126s - loss: 0.4407 - acc: 0.7883 - _logloss: 0.4407 - val_loss: 0.5340 - val_acc: 0.7428 - val__logloss: 0.5340\n",
      "Epoch 9/10\n",
      "126s - loss: 0.4306 - acc: 0.7939 - _logloss: 0.4306 - val_loss: 0.5554 - val_acc: 0.7419 - val__logloss: 0.5554\n",
      "Epoch 10/10\n",
      "126s - loss: 0.4199 - acc: 0.7986 - _logloss: 0.4199 - val_loss: 0.5546 - val_acc: 0.7416 - val__logloss: 0.5546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21d121df940>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print('Train...')\n",
    "N=20000 * 5\n",
    "model.fit([q1_train[:N,:], q2_train[:N,:]], y_train[:N], epochs=10, batch_size=128, validation_data=([q1_dev, q2_dev], y_dev), verbose=2)\n",
    "# score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
